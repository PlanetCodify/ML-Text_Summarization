{"cells":[{"cell_type":"markdown","metadata":{"id":"Sgv5xFznP8C_"},"source":["# Summarization\n","## This notebook outlines the concepts behind Text Summarization"]},{"cell_type":"markdown","metadata":{"id":"6f2lPMLSP8DC"},"source":["## Summarization\n","- concept of capturing very important gist of a long piece of text\n","\n","### Types of Summarization\n","- 1. **Extractive Summarization**\n","    - Select sentences from the corpus that best represent the text\n","    - Arrange them to form a summary\n","- 2. **Abstractive Summarization**\n","    - Captures the very important sentences from the text\n","    - Paraphrases them to form a summary"]},{"cell_type":"markdown","metadata":{"id":"gcv-PvDCP8DC"},"source":["## Summarization Libraries\n","- Sumy\n","- Summa\n","\n","\n","\n","Documentation Reference [sumy](https://github.com/miso-belica/sumy)"]},{"cell_type":"markdown","metadata":{"id":"4NC6GsOlP8DD"},"source":["## Task: Take a piece of text from wiki page and summarize them using Sumy\n","### Steps\n","- Install the necessary libraries\n","- Import the libraries\n","- Scrape the text from a pre-defined webpage\n","- Summarize"]},{"cell_type":"markdown","metadata":{"id":"Nu0uuhn_P8DD"},"source":["### Install Sumy"]},{"cell_type":"code","execution_count":1,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"GlyFH79aP8DD","executionInfo":{"status":"ok","timestamp":1737850887180,"user_tz":300,"elapsed":16819,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"edd621fa-592d-4e0a-f754-fc235798a8a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sumy\n","  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n","Collecting docopt<0.7,>=0.6.1 (from sumy)\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting breadability>=0.1.20 (from sumy)\n","  Downloading breadability-0.1.20.tar.gz (32 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n","Collecting pycountry>=18.2.23 (from sumy)\n","  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n","Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.3.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2024.12.14)\n","Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n","  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=a9c620318d5a5319c455e9f07bb0c66e6c25079f9844075c634da7f5cb3ffa80\n","  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=37de26d80f0aa8be4a4a2a080a9864d5e84752b66b1fcc1c507559f186470eee\n","  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n","Successfully built breadability docopt\n","Installing collected packages: docopt, pycountry, breadability, sumy\n","Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"]}],"source":["! pip install sumy"]},{"cell_type":"code","source":["!pip install lxml_html_clean"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HAuj39aYMha","executionInfo":{"status":"ok","timestamp":1737850900571,"user_tz":300,"elapsed":6528,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"d4f53b45-57f5-400f-8d59-bc7a971ad9a5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lxml_html_clean\n","  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.0)\n","Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n","Installing collected packages: lxml_html_clean\n","Successfully installed lxml_html_clean-0.4.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"Nn0pAPFIP8DE"},"source":["### Import the libraries\n","- HtmlParser\n","- Tokenizer\n","- TextRankSummarizer"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"58gRgUhjP8DE","executionInfo":{"status":"ok","timestamp":1737850914035,"user_tz":300,"elapsed":10682,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}}},"outputs":[],"source":["from sumy.parsers.html import HtmlParser\n","from sumy.nlp.tokenizers import Tokenizer\n","from sumy.summarizers.text_rank import TextRankSummarizer"]},{"cell_type":"markdown","metadata":{"id":"FhBEloRTP8DF"},"source":["### Scrape the text"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Eb8ilRjXP8DF","executionInfo":{"status":"ok","timestamp":1737850917111,"user_tz":300,"elapsed":234,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}}},"outputs":[],"source":["url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n"]},{"cell_type":"code","source":[" import nltk\n"," nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRX04E08YiYK","executionInfo":{"status":"ok","timestamp":1737850921690,"user_tz":300,"elapsed":1885,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"42524862-13dc-45f7-c2fb-4a061bbdcac8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YFbSZ7jRP8DF","executionInfo":{"status":"ok","timestamp":1737850926467,"user_tz":300,"elapsed":2661,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"b8641932-fa8f-47e3-d31f-77182758a964"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<DOM with 63 paragraphs>"]},"metadata":{},"execution_count":6}],"source":["parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n","doc = parser.document\n","doc"]},{"cell_type":"markdown","metadata":{"id":"Kg_RJPmyP8DF"},"source":["### Summarize - TextRankSummarizer"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"uBPatLIfP8DF","executionInfo":{"status":"ok","timestamp":1737850930247,"user_tz":300,"elapsed":819,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}}},"outputs":[],"source":["summarizer = TextRankSummarizer()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85hkMyc9P8DF","executionInfo":{"status":"ok","timestamp":1737850935801,"user_tz":300,"elapsed":3759,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"f85e147f-e6ea-4e23-db3c-a0b45b2ee69b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<Sentence: For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.>,\n"," <Sentence: Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[16] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.>,\n"," <Sentence: Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).>,\n"," <Sentence: While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.>,\n"," <Sentence: A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.>)"]},"metadata":{},"execution_count":8}],"source":["summary_text = summarizer(doc, 5)\n","summary_text"]},{"cell_type":"markdown","metadata":{"id":"CbS9e8toP8DG"},"source":["### Try different Summarizers\n","- LexRankSummarizer\n","- LuhnSummarizer\n","- LsaSummarizer"]},{"cell_type":"markdown","metadata":{"id":"TN9KOOTkP8DG"},"source":["### Import the summarizers"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"_a7lgHEBP8DG","executionInfo":{"status":"ok","timestamp":1737850939473,"user_tz":300,"elapsed":797,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}}},"outputs":[],"source":["from sumy.summarizers.lex_rank import LexRankSummarizer\n","from sumy.summarizers.luhn import LuhnSummarizer\n","from sumy.summarizers.lsa import LsaSummarizer"]},{"cell_type":"markdown","metadata":{"id":"GgsudCaOP8DG"},"source":["### Create Summarizers"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"g36PWU7YP8DG","executionInfo":{"status":"ok","timestamp":1737850939961,"user_tz":300,"elapsed":2,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}}},"outputs":[],"source":["lexSummarizer =  LexRankSummarizer()\n","luhnSummarizer = LuhnSummarizer()\n","lsaSummarizer = LsaSummarizer()"]},{"cell_type":"markdown","metadata":{"id":"lV8UZfEZP8DG"},"source":["### LexRankSummarizer"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CUUspPwP8DG","executionInfo":{"status":"ok","timestamp":1737850946321,"user_tz":300,"elapsed":3720,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"82184e00-fa3f-4062-d6b2-5d841200d1d0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<Sentence: An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.>,\n"," <Sentence: The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".>,\n"," <Sentence: The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.>,\n"," <Sentence: For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.>,\n"," <Sentence: Automatic Text Summarization.>)"]},"metadata":{},"execution_count":11}],"source":["lex_summary_text = lexSummarizer(doc, 5)\n","lex_summary_text"]},{"cell_type":"markdown","metadata":{"id":"RCSn3xUBP8DG"},"source":["### LuhnSummarizer"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynM3IoscP8DG","executionInfo":{"status":"ok","timestamp":1737850949261,"user_tz":300,"elapsed":802,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"d70a695b-0c62-41fe-bd99-519ae393967a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<Sentence: Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.>,\n"," <Sentence: Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).>,\n"," <Sentence: For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.>,\n"," <Sentence: It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ( MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.>,\n"," <Sentence: A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.>)"]},"metadata":{},"execution_count":12}],"source":["luhn_summary_text = luhnSummarizer(doc, 5)\n","luhn_summary_text"]},{"cell_type":"markdown","metadata":{"id":"MB39gyhsP8DG"},"source":["### LsaSummarizer"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezFUKlyoP8DH","executionInfo":{"status":"ok","timestamp":1737850953059,"user_tz":300,"elapsed":1148,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"dd8ed618-894c-4b0b-c4f5-3159b95c41c3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<Sentence: For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.>,\n"," <Sentence: Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.>,\n"," <Sentence: It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ( MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.>,\n"," <Sentence: Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.>,\n"," <Sentence: Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.>)"]},"metadata":{},"execution_count":13}],"source":["lsa_summary_text = lsaSummarizer(doc, 5)\n","lsa_summary_text"]},{"cell_type":"markdown","metadata":{"id":"40TBU9j0P8DN"},"source":["## 2. Summa"]},{"cell_type":"markdown","metadata":{"id":"AdPkgE5FP8DN"},"source":["## Task: Take a piece of text from wiki page and summarize them using Gensim\n","### Steps\n","- Install the necessary libraries\n","- Import the libraries\n","- Scrape the text from a pre-defined webpage\n","- Summarize"]},{"cell_type":"markdown","metadata":{"id":"hiSetzIBP8DN"},"source":["### Install the library"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Gk5bztxUP8DN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737850960910,"user_tz":300,"elapsed":5271,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"4adbce7a-7c32-4914-8547-a833de28314d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting summa\n","  Downloading summa-1.2.0.tar.gz (54 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.11/dist-packages (from summa) (1.13.1)\n","Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy>=0.19->summa) (1.26.4)\n","Building wheels for collected packages: summa\n","  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54387 sha256=7c44457cf2c33c589e0454f393b835362a47aa489a1a8b77b087456815fd33b4\n","  Stored in directory: /root/.cache/pip/wheels/10/2d/7a/abce87c4ea233f8dcca0d99b740ac0257eced1f99a124a0e1f\n","Successfully built summa\n","Installing collected packages: summa\n","Successfully installed summa-1.2.0\n"]}],"source":["# !pip install summa\n","!pip install summa"]},{"cell_type":"markdown","metadata":{"id":"xRTcIWNOP8DN"},"source":["### Import the library"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"31R5gI9wP8DN","executionInfo":{"status":"ok","timestamp":1737850963331,"user_tz":300,"elapsed":274,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}}},"outputs":[],"source":["from summa import summarizer\n","from summa import keywords"]},{"cell_type":"markdown","source":["Scrape the text"],"metadata":{"id":"6dGP8EXo-mrT"}},{"cell_type":"code","source":["import requests\n","import re\n","from bs4 import BeautifulSoup"],"metadata":{"id":"ghEtx3vq-nws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_page(url):\n","    res = requests.get(url)\n","    soup = BeautifulSoup(res.text, 'html.parser')\n","    return soup"],"metadata":{"id":"mQuU5DdG-p0r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collect_text(soup):\n","    text = f'url: {url}\\n\\n'\n","    para_text = soup.find_all('p')\n","    print(f\"paragraphs text = \\n {para_text}\")\n","    for para in para_text:\n","        text += f\"{para.text}\\n\\n\"\n","    return text"],"metadata":{"id":"Qsuiv148-rX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["url = \"https://en.wikipedia.org/wiki/Automatic_summarization\""],"metadata":{"id":"9CiylaUc-tGb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = collect_text(get_page(url))\n","text"],"metadata":{"id":"tQqI5CnK-u-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i50nKAKUP8DN"},"source":["### Summarize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTDbPWysP8DN","colab":{"base_uri":"https://localhost:8080/","height":268},"executionInfo":{"status":"ok","timestamp":1731790402096,"user_tz":300,"elapsed":1187,"user":{"displayName":"AI Pranav","userId":"06600844365112706109"}},"outputId":"7a9885c1-2798-44b6-f903-4d81bd81f91c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\\nText summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document.[1] On the other hand, visual content can be summarized using computer vision algorithms.\\nImage summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.[2][3][4] Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion.[5][6][7][8] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\\nExamples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.\\nFor text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[11]\\nThe first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).\\nSummarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\\nAn example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\\nSome techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\\nInstead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[16] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.\\nFor example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.\\nLike keyphrase extraction, document summarization aims to identify the essence of a text.\\nSupervised text summarization is very much like supervised keyphrase extraction.\\nThe main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\\nDuring the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain.\\nThe unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.\\nSome unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.\\nThe two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\\nIt is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.\\nMulti-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.\\nThese methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[23] Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[24]\\nA new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position.\\nFor example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.\\nFor example, work by Lin and Bilmes, 2012[26] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.\\nIt is very common for summarization and translation systems in NIST\\'s Document Understanding Conferences.[2] ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references.\\nAlthough they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}],"source":["summa_summary_text = summarizer.summarize(text, ratio=0.1)\n","summa_summary_text"]}],"metadata":{"kernelspec":{"display_name":"testing","language":"python","name":"testing"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}